for(i in 1:length(SR)){
# Create vector for all samples along entire shell length by applying constant sampling resolution
D <- seq(SR[i], sum(GR), SR[i])
# Calculate virtual data
newdata <- carbmodel(Ty, SST, GR, d18Osw, D, AV=TRUE)
# Increase length of new data to match the storage dataframe
if(nrow(newdata) < nrow(Case26)){
newdata <- rbind(newdata, matrix(NA, ncol = ncol(newdata), nrow = nrow(Case26) - nrow(newdata)))
}
newdata <- cbind(Case26$column, newdata)
# Add the new data to the storage dataframe
Case26 <- cbind(Case26, newdata)
}
Case26$column <- NULL
colnames(Case26)[seq(1, 26, 5)] <- paste("SR_", SR)
save(Case26, file = "Case26.rda")
# Case 27: Control data with less time (3 years)
# Set boundary conditions
Td <- seq(1,3*365,1) # Create timeline of 12 years in days
Ty <- Td/365 # Convert to years
MAT<-20 # Set mean annual temperature
Amp<-10 # Set seasonal amplitude
Sext<-2*Amp # Calculate extent of seasonal variability
TSD<-1.5 # Set the degree of random non-seasonal noise on the SST curve ("weather")
SST<-rnorm(length(Ty),MAT+Amp*sin(2*pi*Ty),TSD) # Create virtual daily SST data
GR<-rep(10/365,length(Ty)) # Set growth rate to 10 mm/yr, create daily GR vector
DSD<-0.6 # Set the degree of random non-seasonal noise on the d18Osw curve ("salinity fluctuations")
d18Osw<-rnorm(length(Ty),rep(0,length(Ty)),DSD) # Set d18Osw to 0 permille VSMOW, create daily d18Osw vector
SR<-as.vector(c(0.1,0.2,0.45,0.75,1.55,3.25)) # Set sampling resolutions at 3.3 mm (~3 yr-1), 1.55 mm (~6 yr-1; bimonthly), 0.75 mm (~12 yr-1; monthly), 0.45 mm (~25 yr-1), 0.2 mm (~50 yr-1) and 0.1 mm (~100 yr-1, maximum isotope sampling)
# Sampling resolutions for courser sampling are deliberately chosen as non-multiples of the growth rate (irregular numbers) to prevent bias against some months
# Loop through vector and calculate D, d18Oc and D47 data for all sampling densities
Case27 <- data.frame(column = rep(NA, sum(GR) / SR[1]))
for(i in 1:length(SR)){
# Create vector for all samples along entire shell length by applying constant sampling resolution
D <- seq(SR[i], sum(GR), SR[i])
# Calculate virtual data
newdata <- carbmodel(Ty, SST, GR, d18Osw, D, AV=TRUE)
# Increase length of new data to match the storage dataframe
if(nrow(newdata) < nrow(Case27)){
newdata <- rbind(newdata, matrix(NA, ncol = ncol(newdata), nrow = nrow(Case27) - nrow(newdata)))
}
newdata <- cbind(Case27$column, newdata)
# Add the new data to the storage dataframe
Case27 <- cbind(Case27, newdata)
}
Case27$column <- NULL
colnames(Case27)[seq(1, 26, 5)] <- paste("SR_", SR)
save(Case27, file = "Case27.rda")
# Case 28: Control data with less time (1 year)
# Set boundary conditions
Td <- seq(1,365,1) # Create timeline of 12 years in days
Ty <- Td/365 # Convert to years
MAT<-20 # Set mean annual temperature
Amp<-10 # Set seasonal amplitude
Sext<-2*Amp # Calculate extent of seasonal variability
TSD<-1.5 # Set the degree of random non-seasonal noise on the SST curve ("weather")
SST<-rnorm(length(Ty),MAT+Amp*sin(2*pi*Ty),TSD) # Create virtual daily SST data
GR<-rep(10/365,length(Ty)) # Set growth rate to 10 mm/yr, create daily GR vector
DSD<-0.6 # Set the degree of random non-seasonal noise on the d18Osw curve ("salinity fluctuations")
d18Osw<-rnorm(length(Ty),rep(0,length(Ty)),DSD) # Set d18Osw to 0 permille VSMOW, create daily d18Osw vector
SR<-as.vector(c(0.1,0.2,0.45,0.75,1.55,3.25)) # Set sampling resolutions at 3.3 mm (~3 yr-1), 1.55 mm (~6 yr-1; bimonthly), 0.75 mm (~12 yr-1; monthly), 0.45 mm (~25 yr-1), 0.2 mm (~50 yr-1) and 0.1 mm (~100 yr-1, maximum isotope sampling)
# Sampling resolutions for courser sampling are deliberately chosen as non-multiples of the growth rate (irregular numbers) to prevent bias against some months
# Loop through vector and calculate D, d18Oc and D47 data for all sampling densities
Case28 <- data.frame(column = rep(NA, sum(GR) / SR[1]))
for(i in 1:length(SR)){
# Create vector for all samples along entire shell length by applying constant sampling resolution
D <- seq(SR[i], sum(GR), SR[i])
# Calculate virtual data
newdata <- carbmodel(Ty, SST, GR, d18Osw, D, AV=TRUE)
# Increase length of new data to match the storage dataframe
if(nrow(newdata) < nrow(Case28)){
newdata <- rbind(newdata, matrix(NA, ncol = ncol(newdata), nrow = nrow(Case28) - nrow(newdata)))
}
newdata <- cbind(Case28$column, newdata)
# Add the new data to the storage dataframe
Case28 <- cbind(Case28, newdata)
}
Case28$column <- NULL
colnames(Case28)[seq(1, 26, 5)] <- paste("SR_", SR)
save(Case28, file = "Case28.rda")
# Case 29: Case 9 with tiny (1 days SD) error on age model
# Set boundary conditions
Td <- seq(1,12*365,1) # Create timeline of 12 years in days
AMSD<-1 # Set the uncertainty of the age model (in days)
Ty <- Td/365 # Convert to years
MAT<-20 # Set mean annual temperature
Amp<-10 # Set seasonal amplitude
Sext<-2*Amp # Calculate extent of seasonal variability
TSD<-1.5 # Set the degree of random non-seasonal noise on the SST curve ("weather")
SST<-rnorm(length(Ty),MAT+Amp*sin(2*pi*Ty),TSD) # Create virtual daily SST data
GR<-rep(10/365,length(Ty)) # Set growth rate to 10 mm/yr, create daily GR vector
d18Oswmean<-0 # Set annual average d18O of seawater (0 permille VSMOW)
d18Oswamp<-1 # Set seasonal amplitude of seawater d18O (1 permille)
DSD<-0.6 # Set the degree of random non-seasonal noise on the d18Osw curve ("salinity fluctuations")
d18Osw<-rnorm(length(Ty),d18Oswmean-d18Oswamp*sin(2*pi*Ty),DSD)
SR<-as.vector(c(0.1,0.2,0.45,0.75,1.55,3.25)) # Set sampling resolutions at 3.3 mm (~3 yr-1), 1.55 mm (~6 yr-1; bimonthly), 0.75 mm (~12 yr-1; monthly), 0.45 mm (~25 yr-1), 0.2 mm (~50 yr-1) and 0.1 mm (~100 yr-1, maximum isotope sampling)
# Sampling resolutions for courser sampling are deliberately chosen as non-multiples of the growth rate (irregular numbers) to prevent bias against some months
# Loop through vector and calculate D, d18Oc and D47 data for all sampling densities
Case29 <- data.frame(column = rep(NA, sum(GR) / SR[1]))
for(i in 1:length(SR)){
# Create vector for all samples along entire shell length by applying constant sampling resolution
D <- seq(SR[i], sum(GR), SR[i])
# Calculate virtual data
newdata <- carbmodel(Ty, SST, GR, d18Osw, D, AV=TRUE)
# Increase length of new data to match the storage dataframe
if(nrow(newdata) < nrow(Case29)){
newdata <- rbind(newdata, matrix(NA, ncol = ncol(newdata), nrow = nrow(Case29) - nrow(newdata)))
}
newdata <- cbind(Case29$column, newdata)
# Add the new data to the storage dataframe
Case29 <- cbind(Case29, newdata)
}
Case29$column <- NULL
colnames(Case29)[seq(1, 26, 5)] <- paste("SR_", SR)
save(Case29, file = "Case29.rda")
# Case 30: Case 9 with small (5 days SD) error on age model (roughly +/- one week)
# Set boundary conditions
Td <- seq(1,12*365,1) # Create timeline of 12 years in days
AMSD<-5 # Set the uncertainty of the age model (in days)
Ty <- Td/365 # Convert to years
MAT<-20 # Set mean annual temperature
Amp<-10 # Set seasonal amplitude
Sext<-2*Amp # Calculate extent of seasonal variability
TSD<-1.5 # Set the degree of random non-seasonal noise on the SST curve ("weather")
SST<-rnorm(length(Ty),MAT+Amp*sin(2*pi*Ty),TSD) # Create virtual daily SST data
GR<-rep(10/365,length(Ty)) # Set growth rate to 10 mm/yr, create daily GR vector
d18Oswmean<-0 # Set annual average d18O of seawater (0 permille VSMOW)
d18Oswamp<-1 # Set seasonal amplitude of seawater d18O (1 permille)
DSD<-0.6 # Set the degree of random non-seasonal noise on the d18Osw curve ("salinity fluctuations")
d18Osw<-rnorm(length(Ty),d18Oswmean-d18Oswamp*sin(2*pi*Ty),DSD)
SR<-as.vector(c(0.1,0.2,0.45,0.75,1.55,3.25)) # Set sampling resolutions at 3.3 mm (~3 yr-1), 1.55 mm (~6 yr-1; bimonthly), 0.75 mm (~12 yr-1; monthly), 0.45 mm (~25 yr-1), 0.2 mm (~50 yr-1) and 0.1 mm (~100 yr-1, maximum isotope sampling)
# Sampling resolutions for courser sampling are deliberately chosen as non-multiples of the growth rate (irregular numbers) to prevent bias against some months
# Loop through vector and calculate D, d18Oc and D47 data for all sampling densities
Case30 <- data.frame(column = rep(NA, sum(GR) / SR[1]))
for(i in 1:length(SR)){
# Create vector for all samples along entire shell length by applying constant sampling resolution
D <- seq(SR[i], sum(GR), SR[i])
# Calculate virtual data
newdata <- carbmodel(Ty, SST, GR, d18Osw, D, AV=TRUE)
# Increase length of new data to match the storage dataframe
if(nrow(newdata) < nrow(Case30)){
newdata <- rbind(newdata, matrix(NA, ncol = ncol(newdata), nrow = nrow(Case30) - nrow(newdata)))
}
newdata <- cbind(Case30$column, newdata)
# Add the new data to the storage dataframe
Case30 <- cbind(Case30, newdata)
}
Case30$column <- NULL
colnames(Case30)[seq(1, 26, 5)] <- paste("SR_", SR)
save(Case30, file = "Case30.rda")
# Case 31: Case 9 with larger (15 days SD) error on age model (roughly +/- one month)
# Set boundary conditions
Td <- seq(1,12*365,1) # Create timeline of 12 years in days
AMSD<-15 # Set the uncertainty of the age model (in days)
Ty <- Td/365 # Convert to years
MAT<-20 # Set mean annual temperature
Amp<-10 # Set seasonal amplitude
Sext<-2*Amp # Calculate extent of seasonal variability
TSD<-1.5 # Set the degree of random non-seasonal noise on the SST curve ("weather")
SST<-rnorm(length(Ty),MAT+Amp*sin(2*pi*Ty),TSD) # Create virtual daily SST data
GR<-rep(10/365,length(Ty)) # Set growth rate to 10 mm/yr, create daily GR vector
d18Oswmean<-0 # Set annual average d18O of seawater (0 permille VSMOW)
d18Oswamp<-1 # Set seasonal amplitude of seawater d18O (1 permille)
DSD<-0.6 # Set the degree of random non-seasonal noise on the d18Osw curve ("salinity fluctuations")
d18Osw<-rnorm(length(Ty),d18Oswmean-d18Oswamp*sin(2*pi*Ty),DSD)
SR<-as.vector(c(0.1,0.2,0.45,0.75,1.55,3.25)) # Set sampling resolutions at 3.3 mm (~3 yr-1), 1.55 mm (~6 yr-1; bimonthly), 0.75 mm (~12 yr-1; monthly), 0.45 mm (~25 yr-1), 0.2 mm (~50 yr-1) and 0.1 mm (~100 yr-1, maximum isotope sampling)
# Sampling resolutions for courser sampling are deliberately chosen as non-multiples of the growth rate (irregular numbers) to prevent bias against some months
# Loop through vector and calculate D, d18Oc and D47 data for all sampling densities
Case31 <- data.frame(column = rep(NA, sum(GR) / SR[1]))
for(i in 1:length(SR)){
# Create vector for all samples along entire shell length by applying constant sampling resolution
D <- seq(SR[i], sum(GR), SR[i])
# Calculate virtual data
newdata <- carbmodel(Ty, SST, GR, d18Osw, D, AV=TRUE)
# Increase length of new data to match the storage dataframe
if(nrow(newdata) < nrow(Case31)){
newdata <- rbind(newdata, matrix(NA, ncol = ncol(newdata), nrow = nrow(Case31) - nrow(newdata)))
}
newdata <- cbind(Case31$column, newdata)
# Add the new data to the storage dataframe
Case31 <- cbind(Case31, newdata)
}
Case31$column <- NULL
colnames(Case31)[seq(1, 26, 5)] <- paste("SR_", SR)
save(Case31, file = "Case31.rda")
# Case 32: Case 9 with large (45 days SD) error on age model (roughly +/- three months)
# Set boundary conditions
Td <- seq(1,12*365,1) # Create timeline of 12 years in days
AMSD<-45 # Set the uncertainty of the age model (in days)
Ty <- Td/365 # Convert to years
MAT<-20 # Set mean annual temperature
Amp<-10 # Set seasonal amplitude
Sext<-2*Amp # Calculate extent of seasonal variability
TSD<-1.5 # Set the degree of random non-seasonal noise on the SST curve ("weather")
SST<-rnorm(length(Ty),MAT+Amp*sin(2*pi*Ty),TSD) # Create virtual daily SST data
GR<-rep(10/365,length(Ty)) # Set growth rate to 10 mm/yr, create daily GR vector
DSD<-0.6 # Set the degree of random non-seasonal noise on the d18Osw curve ("salinity fluctuations")
d18Oswmean<-0 # Set annual average d18O of seawater (0 permille VSMOW)
d18Oswamp<-1 # Set seasonal amplitude of seawater d18O (1 permille)
DSD<-0.6 # Set the degree of random non-seasonal noise on the d18Osw curve ("salinity fluctuations")
d18Osw<-rnorm(length(Ty),d18Oswmean-d18Oswamp*sin(2*pi*Ty),DSD)
SR<-as.vector(c(0.1,0.2,0.45,0.75,1.55,3.25)) # Set sampling resolutions at 3.3 mm (~3 yr-1), 1.55 mm (~6 yr-1; bimonthly), 0.75 mm (~12 yr-1; monthly), 0.45 mm (~25 yr-1), 0.2 mm (~50 yr-1) and 0.1 mm (~100 yr-1, maximum isotope sampling)
# Sampling resolutions for courser sampling are deliberately chosen as non-multiples of the growth rate (irregular numbers) to prevent bias against some months
# Loop through vector and calculate D, d18Oc and D47 data for all sampling densities
Case32 <- data.frame(column = rep(NA, sum(GR) / SR[1]))
for(i in 1:length(SR)){
# Create vector for all samples along entire shell length by applying constant sampling resolution
D <- seq(SR[i], sum(GR), SR[i])
# Calculate virtual data
newdata <- carbmodel(Ty, SST, GR, d18Osw, D, AV=TRUE)
# Increase length of new data to match the storage dataframe
if(nrow(newdata) < nrow(Case32)){
newdata <- rbind(newdata, matrix(NA, ncol = ncol(newdata), nrow = nrow(Case32) - nrow(newdata)))
}
newdata <- cbind(Case32$column, newdata)
# Add the new data to the storage dataframe
Case32 <- cbind(Case32, newdata)
}
Case32$column <- NULL
colnames(Case32)[seq(1, 26, 5)] <- paste("SR_", SR)
save(Case32, file = "Case32.rda")
# Case 33: Case 9 with enormous (90 days SD) error on age model (roughly +/- half year)
# Set boundary conditions
Td <- seq(1,12*365,1) # Create timeline of 12 years in days
AMSD<-90 # Set the uncertainty of the age model (in days)
Ty <- Td/365 # Convert to years
MAT<-20 # Set mean annual temperature
Amp<-10 # Set seasonal amplitude
Sext<-2*Amp # Calculate extent of seasonal variability
TSD<-1.5 # Set the degree of random non-seasonal noise on the SST curve ("weather")
SST<-rnorm(length(Ty),MAT+Amp*sin(2*pi*Ty),TSD) # Create virtual daily SST data
GR<-rep(10/365,length(Ty)) # Set growth rate to 10 mm/yr, create daily GR vector
d18Oswmean<-0 # Set annual average d18O of seawater (0 permille VSMOW)
d18Oswamp<-1 # Set seasonal amplitude of seawater d18O (1 permille)
DSD<-0.6 # Set the degree of random non-seasonal noise on the d18Osw curve ("salinity fluctuations")
d18Osw<-rnorm(length(Ty),d18Oswmean-d18Oswamp*sin(2*pi*Ty),DSD)
SR<-as.vector(c(0.1,0.2,0.45,0.75,1.55,3.25)) # Set sampling resolutions at 3.3 mm (~3 yr-1), 1.55 mm (~6 yr-1; bimonthly), 0.75 mm (~12 yr-1; monthly), 0.45 mm (~25 yr-1), 0.2 mm (~50 yr-1) and 0.1 mm (~100 yr-1, maximum isotope sampling)
# Sampling resolutions for courser sampling are deliberately chosen as non-multiples of the growth rate (irregular numbers) to prevent bias against some months
# Loop through vector and calculate D, d18Oc and D47 data for all sampling densities
Case33 <- data.frame(column = rep(NA, sum(GR) / SR[1]))
for(i in 1:length(SR)){
# Create vector for all samples along entire shell length by applying constant sampling resolution
D <- seq(SR[i], sum(GR), SR[i])
# Calculate virtual data
newdata <- carbmodel(Ty, SST, GR, d18Osw, D, AV=TRUE)
# Increase length of new data to match the storage dataframe
if(nrow(newdata) < nrow(Case33)){
newdata <- rbind(newdata, matrix(NA, ncol = ncol(newdata), nrow = nrow(Case33) - nrow(newdata)))
}
newdata <- cbind(Case33$column, newdata)
# Add the new data to the storage dataframe
Case33 <- cbind(Case33, newdata)
}
Case33$column <- NULL
colnames(Case33)[seq(1, 26, 5)] <- paste("SR_", SR)
save(Case33, file = "Case33.rda")
GT <- read.csv("inst/extdata/Texel_data.csv") # From within package root directory
GT <- read.csv("inst/extdata/Texel_data.csv") # From within package root directory
Ty <- GT[, 2]
GR <- GT[, 3]
SST <- GT[, 4]
d18Osw <- GT[, 5]
SR <- as.vector(c(0.1, 0.2, 0.45, 0.75, 1.55, 3.25)) # Set sampling resolutions at 3.3 mm (~3 yr-1), 1.55 mm (~6 yr-1; bimonthly), 0.75 mm (~12 yr-1; monthly), 0.45 mm (~25 yr-1), 0.2 mm (~50 yr-1) and 0.1 mm (~100 yr-1, maximum isotope sampling)
# Sampling resolutions for courser sampling are deliberately chosen as non-multiples of the growth rate (irregular numbers) to prevent bias against some months
# Loop through vector and calculate D, d18Oc and D47 data for all sampling densities
Case22 <- data.frame(column = rep(NA, sum(GR, na.rm = TRUE) / SR[1]))
for(i in 1:length(SR)){
# Create vector for all samples along entire shell length by applying constant sampling resolution
D <- seq(SR[i], sum(GR, na.rm = TRUE), SR[i])
# Calculate virtual data
newdata <- carbmodel(Ty, SST, GR, d18Osw, D, AV=TRUE)
# Increase length of new data to match the storage dataframe
if(nrow(newdata) < nrow(Case22)){
newdata <- rbind(newdata, matrix(NA, ncol = ncol(newdata), nrow = nrow(Case22) - nrow(newdata)))
}
newdata <- cbind(Case22$column, newdata)
# Add the new data to the storage dataframe
Case22 <- cbind(Case22, newdata)
}
Case22$column <- NULL
colnames(Case22)[seq(1, 26, 5)] <- paste("SR_", SR)
save(Case22, file = "data/Case22.rda")
require(seasonalclumped)
GT <- read.csv("inst/extdata/Texel_data.csv") # From within package root directory
Ty <- GT[, 2]
GR <- GT[, 3]
SST <- GT[, 4]
d18Osw <- GT[, 5]
SR <- as.vector(c(0.1, 0.2, 0.45, 0.75, 1.55, 3.25)) # Set sampling resolutions at 3.3 mm (~3 yr-1), 1.55 mm (~6 yr-1; bimonthly), 0.75 mm (~12 yr-1; monthly), 0.45 mm (~25 yr-1), 0.2 mm (~50 yr-1) and 0.1 mm (~100 yr-1, maximum isotope sampling)
# Sampling resolutions for courser sampling are deliberately chosen as non-multiples of the growth rate (irregular numbers) to prevent bias against some months
# Loop through vector and calculate D, d18Oc and D47 data for all sampling densities
Case22 <- data.frame(column = rep(NA, sum(GR, na.rm = TRUE) / SR[1]))
for(i in 1:length(SR)){
# Create vector for all samples along entire shell length by applying constant sampling resolution
D <- seq(SR[i], sum(GR, na.rm = TRUE), SR[i])
# Calculate virtual data
newdata <- carbmodel(Ty, SST, GR, d18Osw, D, AV=TRUE)
# Increase length of new data to match the storage dataframe
if(nrow(newdata) < nrow(Case22)){
newdata <- rbind(newdata, matrix(NA, ncol = ncol(newdata), nrow = nrow(Case22) - nrow(newdata)))
}
newdata <- cbind(Case22$column, newdata)
# Add the new data to the storage dataframe
Case22 <- cbind(Case22, newdata)
}
Case22$column <- NULL
colnames(Case22)[seq(1, 26, 5)] <- paste("SR_", SR)
save(Case22, file = "data/Case22.rda")
GT <- read.csv("inst/extdata/GBR_data.csv") # From within package root directory
Ty <- GT[, 2]
GR <- GT[, 3]
SST <- GT[, 4]
d18Osw <- GT[, 5]
SR <- as.vector(c(0.1, 0.2, 0.45, 0.75, 1.55, 3.25)) # Set sampling resolutions at 3.3 mm (~3 yr-1), 1.55 mm (~6 yr-1; bimonthly), 0.75 mm (~12 yr-1; monthly), 0.45 mm (~25 yr-1), 0.2 mm (~50 yr-1) and 0.1 mm (~100 yr-1, maximum isotope sampling)
# Sampling resolutions for courser sampling are deliberately chosen as non-multiples of the growth rate (irregular numbers) to prevent bias against some months
# Loop through vector and calculate D, d18Oc and D47 data for all sampling densities
Case23 <- data.frame(column = rep(NA, sum(GR, na.rm = TRUE) / SR[1]))
for(i in 1:length(SR)){
# Create vector for all samples along entire shell length by applying constant sampling resolution
D <- seq(SR[i], sum(GR, na.rm = TRUE), SR[i])
# Calculate virtual data
newdata <- carbmodel(Ty, SST, GR, d18Osw, D, AV=TRUE)
# Increase length of new data to match the storage dataframe
if(nrow(newdata) < nrow(Case23)){
newdata <- rbind(newdata, matrix(NA, ncol = ncol(newdata), nrow = nrow(Case23) - nrow(newdata)))
}
newdata <- cbind(Case23$column, newdata)
# Add the new data to the storage dataframe
Case23 <- cbind(Case23, newdata)
}
Case23$column <- NULL
colnames(Case23)[seq(1, 26, 5)] <- paste("SR_", SR)
save(Case23, file = "data/Case23.rda")
GT <- read.csv("inst/extdata/Red_sea_data.csv") # From within package root directory
Ty <- GT[, 2]
GR <- GT[, 3]
SST <- GT[, 4]
d18Osw <- GT[, 5]
SR <- as.vector(c(0.1, 0.2, 0.45, 0.75, 1.55, 3.25)) # Set sampling resolutions at 3.3 mm (~3 yr-1), 1.55 mm (~6 yr-1; bimonthly), 0.75 mm (~12 yr-1; monthly), 0.45 mm (~25 yr-1), 0.2 mm (~50 yr-1) and 0.1 mm (~100 yr-1, maximum isotope sampling)
# Sampling resolutions for courser sampling are deliberately chosen as non-multiples of the growth rate (irregular numbers) to prevent bias against some months
# Loop through vector and calculate D, d18Oc and D47 data for all sampling densities
Case24 <- data.frame(column = rep(NA, sum(GR, na.rm = TRUE) / SR[1]))
for(i in 1:length(SR)){
# Create vector for all samples along entire shell length by applying constant sampling resolution
D <- seq(SR[i], sum(GR, na.rm = TRUE), SR[i])
# Calculate virtual data
newdata <- carbmodel(Ty, SST, GR, d18Osw, D, AV=TRUE)
# Increase length of new data to match the storage dataframe
if(nrow(newdata) < nrow(Case24)){
newdata <- rbind(newdata, matrix(NA, ncol = ncol(newdata), nrow = nrow(Case24) - nrow(newdata)))
}
newdata <- cbind(Case24$column, newdata)
# Add the new data to the storage dataframe
Case24 <- cbind(Case24, newdata)
}
Case24$column <- NULL
colnames(Case24)[seq(1, 26, 5)] <- paste("SR_", SR)
save(Case24, file = "data/Case24.rda")
GT <- read.csv("inst/extdata/Iceland_data.csv") # From within package root directory
Ty <- GT[, 2]
GR <- GT[, 3]
SST <- GT[, 4]
d18Osw <- GT[, 5]
SR <- as.vector(c(0.1, 0.2, 0.45, 0.75, 1.55, 3.25)) # Set sampling resolutions at 3.3 mm (~3 yr-1), 1.55 mm (~6 yr-1; bimonthly), 0.75 mm (~12 yr-1; monthly), 0.45 mm (~25 yr-1), 0.2 mm (~50 yr-1) and 0.1 mm (~100 yr-1, maximum isotope sampling)
# Sampling resolutions for courser sampling are deliberately chosen as non-multiples of the growth rate (irregular numbers) to prevent bias against some months
# Loop through vector and calculate D, d18Oc and D47 data for all sampling densities
Case25 <- data.frame(column = rep(NA, sum(GR, na.rm = TRUE) / SR[1]))
for(i in 1:length(SR)){
# Create vector for all samples along entire shell length by applying constant sampling resolution
D <- seq(SR[i], sum(GR, na.rm = TRUE), SR[i])
# Calculate virtual data
newdata <- carbmodel(Ty, SST, GR, d18Osw, D, AV=TRUE)
# Increase length of new data to match the storage dataframe
if(nrow(newdata) < nrow(Case25)){
newdata <- rbind(newdata, matrix(NA, ncol = ncol(newdata), nrow = nrow(Case25) - nrow(newdata)))
}
newdata <- cbind(Case25$column, newdata)
# Add the new data to the storage dataframe
Case25 <- cbind(Case25, newdata)
}
Case25$column <- NULL
colnames(Case25)[seq(1, 26, 5)] <- paste("SR_", SR)
save(Case25, file = "data/Case25.rda")
load("E:/Dropbox/Research/postdoc/UNBIAS/seasonalclumped/data/Case1.rda")
rm(list=ls())
load("E:/Dropbox/Research/postdoc/UNBIAS/seasonalclumped/data/Case1.rda")
d18Oc <- Case1[, 29]
d18Oc <- d18Oc[-which(is.na(d18Oc))]
D47 <- Case1[, 30]
D47 <- D47[-which(is.na(D47))]
ages <- Case1[, 27]
ages <- ages[-which(is.na(ages))]
View(Case1)
SD_d18Oc = 0.1 # Error (1 SD) on d18Oc data
SD_D47 = 0.04 # Error (1 SD) on D47 data
N = 1000 # Number of Monte Carlo simulations for optimization
p = 0.05 # p-value threshold for considering successful separation of seasons
d18O_fun = "KimONeil97"
D47_fun = "Bernasconi18"
export = FALSE # Should the result be exported?
export_raw = FALSE
if(length(unique(c(length(d18Oc), length(D47), length(ages)))) > 1){
return("ERROR: Vectors 'd18Oc', 'D47' and 'ages' should have equal length")
}
Popt <- matrix(NA, ncol = 5, nrow = N * length(d18Oc)) # Create matrix with maximum length
colnames(Popt) <- c("Samplesize",
"dOsum",
"dOwin",
"Dsum",
"Dwin") # Create template for storing simulation results
if(length(SD_d18Oc) == 1){
SD_d18Oc <- rep(SD_d18Oc, length(d18Oc)) # Duplicate SD of d18Oc error through entire record length if only a single value is given (constant uncertainty)
}
if(length(SD_D47) == 1){
SD_D47 <- rep(SD_D47, length(D47)) # Duplicate SD of D47 error through entire record length if only a single value is given (constant uncertainty)
}
d18Omat <- as.data.frame(matrix(rnorm(N * length(d18Oc), d18Oc, SD_d18Oc), ncol = N)) # Randomly resample d18O data using measurement uncertainty
colnames(d18Omat) <- paste("Sim", seq(1, N, 1), sep = "")
D47mat <- as.data.frame(matrix(rnorm(N * length(D47), D47, SD_D47), ncol = N)) # Randomly resample D47 data using measurement uncertainty
colnames(D47mat) <- paste("Sim", seq(1, N, 1), sep = "")
max(ages)
ceiling(max(ages))
floor(range(ages))
floor(diff(range(ages))
)
ceiling(diff(range(ages))
)
?runMean
require(TTR)
?runMean
?apply
if(length(d18Oc) / ceiling(diff(range(ages))) < 2){ # If there are less than 2 datapoints per year, the window is allowed to become as long as the entire record
win <- seq(1, length(d18Oc), 1)
}else{
win <- seq(1, length(d18O_mod) / ceiling(diff(range(ages))), 1) # Create vector of sample size windows (max sample size is 1 year)
}
if(length(d18Oc) / ceiling(diff(range(ages))) < 2){ # If there are less than 2 datapoints per year, the window is allowed to become as long as the entire record
win <- seq(1, length(d18Oc), 1)
}else{
win <- seq(1, length(d18Oc) / ceiling(diff(range(ages))), 1) # Create vector of sample size windows (max sample size is 1 year)
}
Dwin <- apply(d18Omat, 2, max(runMean(win)))
View(d18Omat)
Dwin <- apply(d18Omat, 2, max(runMean(, win)))
Dwin <- apply(d18Omat, 2, function(x) max(runMean(x, win)))
Dwin <- apply(D47mat, 2, function(x) max(TTR::runMean(x, win))) # Find highest D47 values (winter)
Dsum <- apply(D47mat, 2, function(x) min(TTR::runMean(x, win))) # Find lowest D47 values (summer)
dwin <- apply(d18Omat, 2, function(x) max(TTR::runMean(x, win))) # Find highest d18O values (winter)
dsum <- apply(d18Omat, 2, function(x) min(TTR::runMean(x, win))) # Find lowest d18O values (summer)
Dwinsd <- apply(D47mat, 2, function(x) TTR::runSD(x, win)[which.max(TTR::runMean(x, win))])
?runMean
Dwinsd <- apply(D47mat, 2, function(x) TTR::runSD(x, win)[which.max(TTR::runMean(x, win), na.rm = TRUE)])
if(length(d18Oc) / ceiling(diff(range(ages))) < 2){ # If there are less than 2 datapoints per year, the window is allowed to become as long as the entire record
win <- seq(2, length(d18Oc), 1)
}else{
win <- seq(2, length(d18Oc) / ceiling(diff(range(ages))), 1) # Create vector of sample size windows (max sample size is 1 year)
}
Dwin <- apply(D47mat, 2, function(x) max(TTR::runMean(x, win))) # Find highest D47 values (winter)
Dsum <- apply(D47mat, 2, function(x) min(TTR::runMean(x, win))) # Find lowest D47 values (summer)
dwin <- apply(d18Omat, 2, function(x) max(TTR::runMean(x, win))) # Find highest d18O values (winter)
dsum <- apply(d18Omat, 2, function(x) min(TTR::runMean(x, win))) # Find lowest d18O values (summer)
Dwin <- apply(D47mat, 2, function(x) max(TTR::runMean(x, win), na.rm = TRUE))
Dwin <- apply(D47mat, 2, function(x) max(TTR::runMean(x, win), na.rm = TRUE)) # Find highest D47 values (winter)
Dsum <- apply(D47mat, 2, function(x) min(TTR::runMean(x, win), na.rm = TRUE)) # Find lowest D47 values (summer)
dwin <- apply(d18Omat, 2, function(x) max(TTR::runMean(x, win), na.rm = TRUE)) # Find highest d18O values (winter)
dsum <- apply(d18Omat, 2, function(x) min(TTR::runMean(x, win), na.rm = TRUE)) # Find lowest d18O values (summer)
Dwinsd <- apply(D47mat, 2, function(x) TTR::runSD(x, win)[which.max(TTR::runMean(x, win))])
Dsumsd <- apply(D47mat, 2, function(x) TTR::runSD(x, win)[which.min(TTR::runMean(x, win))]) # Find standard deviation of summer D47 values
x11(9))); plot(Dwinsd)
x11(); plot(Dwinsd)
x11(); plot(Dsumsd)
dwinsd <- apply(d18Omat, 2, function(x) TTR::runSD(x, win)[which.max(TTR::runMean(x, win))]) # Find standard deviation of winter d18O values
dsumsd <- apply(d18Omat, 2, function(x) TTR::runSD(x, win)[which.min(TTR::runMean(x, win))]) # Find standard deviation of summer d18O values
Dwin2 <- apply(D47mat, 2, function(x) max(TTR::runMean(x, 2), na.rm = TRUE))
Dwin3 <- apply(D47mat, 2, function(x) max(TTR::runMean(x, 3), na.rm = TRUE))
Dwin3 <- apply(D47mat, 2, function(x) max(TTR::runMean(x, 1), na.rm = TRUE))
Dwin3 <- apply(D47mat, 2, function(x) max(TTR::runMean(x, 10), na.rm = TRUE))
test <- 1:10
runMean(test, win)
runMean(test, 2)
runMean(test, 3)
runMean(test, 3, cumulative = TRUE)
runMean(test, 3)
runMean(test, 2)
runMean(test, 2.5)
recond <- ages # Create matrix for d18O results
reconD <- ages # Create matrix for D47 results
for(i in win){
# Progress
cat(paste("Optimizing moving average window size: ",i),"\r")
flush.console()
Dwin <- apply(D47mat, 2, function(x) max(TTR::runMean(x, i), na.rm = TRUE)) # Find highest D47 values (winter)
Dsum <- apply(D47mat, 2, function(x) min(TTR::runMean(x, i), na.rm = TRUE)) # Find lowest D47 values (summer)
dwin <- apply(d18Omat, 2, function(x) max(TTR::runMean(x, i), na.rm = TRUE)) # Find highest d18O values (winter)
dsum <- apply(d18Omat, 2, function(x) min(TTR::runMean(x, i), na.rm = TRUE)) # Find lowest d18O values (summer)
Dwinsd <- apply(D47mat, 2, function(x) TTR::runSD(x, i)[which.max(TTR::runMean(x, i))]) # Find standard deviation of winter D47 values
Dsumsd <- apply(D47mat, 2, function(x) TTR::runSD(x, i)[which.min(TTR::runMean(x, i))]) # Find standard deviation of summer D47 values
dwinsd <- apply(d18Omat, 2, function(x) TTR::runSD(x, i)[which.max(TTR::runMean(x, i))]) # Find standard deviation of winter d18O values
dsumsd <- apply(d18Omat, 2, function(x) TTR::runSD(x, i)[which.min(TTR::runMean(x, i))]) # Find standard deviation of summer d18O values
SDpool <- sqrt((Dsumsd ^ 2 + Dwinsd ^ 2) / 2) # Calculate pooled standard deviation for each simulation
T <- (Dsum - Dwin) / (SDpool * sqrt(2 / i)) # Calculate two-sample T-value for each simulation (equal sample size, equal variance)
Pval <- pt(T, i - 1) # Calculate p-value for each window
Popt <- rbind(Popt, c(rep(i, length(which(Pval < p))),
length(which(Pval < p)),
min(Pval, na.rm = TRUE),
dsum[which(Pval < p)],
dwin[which(Pval < p)],
Dsum[which(Pval < p)],
Dwin[which(Pval < p)]))
}
View(Popt)
length(which(!is.na(Popt)))
